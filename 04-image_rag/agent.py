"""
åŸºäºAgentæ¶æ„çš„æº¯æºå¤šæ¨¡æ€RAGç³»ç»Ÿ
ä½¿ç”¨SequentialAgentç»„ç»‡RAGæµç¨‹ï¼šæ•°æ®åŠ è½½ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ â†’ å›ç­”ç”Ÿæˆ
"""

import asyncio
import os
import json
from datetime import datetime
from typing import List, Dict, Any
import numpy as np
import faiss
import dashscope
from google.adk.agents import Agent, SequentialAgent, ParallelAgent
from google.adk.models.lite_llm import LiteLlm
from google.genai import types
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService

from dotenv import load_dotenv
load_dotenv(override=True)

# é…ç½®
QWEN_MODEL = os.getenv("QWEN_MODEL_NAME")
QWEN_API_KEY = os.getenv("QWEN_API_KEY")
QWEN_BASE_URL = os.getenv("QWEN_BASE_URL")
DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL_NAME")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL")

# é…ç½®
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY") or os.getenv("QWEN_API_KEY")
dashscope.api_key = DASHSCOPE_API_KEY

APP_NAME = "rag_agents_system"
USER_ID = "user"

# å…¨å±€FAISSå­˜å‚¨
_FAISS_STORAGE = {
    "text_index": None,
    "insight_index": None,
    "text_chunks": None,
    "insights": None,
    "rag_data": None
}

# ================ å·¥å…·å‡½æ•°å®šä¹‰ ================

def load_rag_data_tool(json_data: str) -> Dict[str, Any]:
    """RAGæ•°æ®åŠ è½½å·¥å…· - å¤„ç†JSONå­—ç¬¦ä¸²"""
    print(f"[å·¥å…·] åŠ è½½RAGæ•°æ®ï¼Œæ•°æ®é•¿åº¦: {len(json_data)} å­—ç¬¦")
    
    try:
        # è§£æJSONå­—ç¬¦ä¸²
        data = json.loads(json_data)
        
        # ç»Ÿè®¡æ•°æ®é‡
        text_count = len(data.get("text_chunks_with_tracing", []))
        table_count = len(data.get("table_data", []))
        chart_count = len(data.get("chart_data", []))
        insights_count = len(data.get("insights_with_source", []))
        
        load_result = {
            "load_status": "success",
            "data_summary": {
                "text_chunks_count": text_count,
                "table_count": table_count,
                "chart_count": chart_count,
                "insights_count": insights_count
            },
            "document_info": {
                "document_type": data.get("document_classification", {}).get("document_name", "æœªçŸ¥"),
                "summary": data.get("document_summary", "æ— æ¦‚è¦"),
                "confidence": data.get("document_classification", {}).get("confidence", 0.0)
            },
            "rag_data": data,  # å®Œæ•´æ•°æ®å­˜å‚¨
            "ready_for_vectorization": True,
            "message": f"æ•°æ®åŠ è½½æˆåŠŸ: {text_count}ä¸ªæ–‡æœ¬å—, {table_count}ä¸ªè¡¨æ ¼, {insights_count}ä¸ªæ´å¯Ÿ"
        }
        
        print(f"[å·¥å…·] æ•°æ®åŠ è½½æˆåŠŸ: {text_count}ä¸ªæ–‡æœ¬å—")
        return load_result
        
    except Exception as e:
        print(f"[å·¥å…·] æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        return {
            "load_status": "failed",
            "error": str(e),
            "ready_for_vectorization": False
        }

def vectorize_content_tool(rag_data: Dict[str, Any]) -> Dict[str, Any]:
    """å†…å®¹å‘é‡åŒ–å·¥å…·"""
    global _FAISS_STORAGE
    print(f"[å·¥å…·] å¼€å§‹FAISSå‘é‡åŒ–å¤„ç†...")
    
    try:
        vectorization_result = {
            "vectorization_status": "success",
            "vectorized_content": {},
            "indexing_complete": True,
            "ready_for_search": True
        }
        
        # å­˜å‚¨RAGæ•°æ®åˆ°å…¨å±€å˜é‡
        _FAISS_STORAGE["rag_data"] = rag_data
        
        # 1. å‘é‡åŒ–text_chunks_with_tracingå¹¶åˆ›å»ºFAISSç´¢å¼•
        if "text_chunks_with_tracing" in rag_data:
            chunks = rag_data["text_chunks_with_tracing"]
            if chunks:
                print(f"[FAISS] å¤„ç† {len(chunks)} ä¸ªæ–‡æœ¬å—...")
                text_contents = [chunk["content"] for chunk in chunks]
                text_embeddings = get_bailian_embedding(text_contents)
                
                # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32ï¼ˆFAISSè¦æ±‚ï¼‰
                text_embeddings = text_embeddings.astype(np.float32)
                
                # éªŒè¯å‘é‡æ•°æ®
                if np.any(np.isnan(text_embeddings)) or np.any(np.isinf(text_embeddings)):
                    raise ValueError("å‘é‡åŒ…å«NaNæˆ–æ— ç©·å¤§å€¼")
                
                # åˆ›å»ºFAISSç´¢å¼• (ä½¿ç”¨å†…ç§¯è®¡ç®—ï¼Œé€‚åˆå½’ä¸€åŒ–å‘é‡)
                dimension = text_embeddings.shape[1]
                text_index = faiss.IndexFlatIP(dimension)
                
                # å‘é‡å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
                faiss.normalize_L2(text_embeddings)
                
                # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
                text_index.add(text_embeddings)
                
                # ä¿å­˜åˆ°å…¨å±€å­˜å‚¨ï¼ˆé¿å…åºåˆ—åŒ–é—®é¢˜ï¼‰
                _FAISS_STORAGE["text_index"] = text_index
                _FAISS_STORAGE["text_chunks"] = chunks
                vectorization_result["vectorized_content"]["text_chunks_vectorized"] = len(chunks)
                
                print(f"[FAISS] æ–‡æœ¬ç´¢å¼•åˆ›å»ºå®Œæˆ: {len(chunks)}ä¸ªå‘é‡, ç»´åº¦: {dimension}")
        
        # 2. å‘é‡åŒ–insights_with_sourceå¹¶åˆ›å»ºFAISSç´¢å¼•
        if "insights_with_source" in rag_data:
            insights = rag_data["insights_with_source"]
            if insights:
                print(f"[FAISS] å¤„ç† {len(insights)} ä¸ªæ´å¯Ÿ...")
                insight_texts = [insight["insight"] for insight in insights]
                insight_embeddings = get_bailian_embedding(insight_texts)
                
                # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32ï¼ˆFAISSè¦æ±‚ï¼‰
                insight_embeddings = insight_embeddings.astype(np.float32)
                
                # éªŒè¯å‘é‡æ•°æ®
                if np.any(np.isnan(insight_embeddings)) or np.any(np.isinf(insight_embeddings)):
                    raise ValueError("æ´å¯Ÿå‘é‡åŒ…å«NaNæˆ–æ— ç©·å¤§å€¼")
                
                # åˆ›å»ºFAISSç´¢å¼•
                dimension = insight_embeddings.shape[1]
                insight_index = faiss.IndexFlatIP(dimension)
                
                # å‘é‡å½’ä¸€åŒ–
                faiss.normalize_L2(insight_embeddings)
                
                # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
                insight_index.add(insight_embeddings)
                
                # ä¿å­˜åˆ°å…¨å±€å­˜å‚¨ï¼ˆé¿å…åºåˆ—åŒ–é—®é¢˜ï¼‰
                _FAISS_STORAGE["insight_index"] = insight_index
                _FAISS_STORAGE["insights"] = insights
                vectorization_result["vectorized_content"]["insights_vectorized"] = len(insights)
                
                print(f"[FAISS] æ´å¯Ÿç´¢å¼•åˆ›å»ºå®Œæˆ: {len(insights)}ä¸ªå‘é‡, ç»´åº¦: {dimension}")
        
        vectorization_result["vectorized_content"]["embedding_model"] = "bailian-text-embedding-v1"
        vectorization_result["vectorized_content"]["vector_dimension"] = dimension if 'dimension' in locals() else 1536
        vectorization_result["vectorized_content"]["index_type"] = "FAISS_IndexFlatIP"
        vectorization_result["vectorized_content"]["storage_method"] = "global_storage"
        vectorization_result["message"] = "FAISSå‘é‡åŒ–å®Œæˆï¼Œç´¢å¼•å·²å­˜å‚¨åˆ°å…¨å±€ç©ºé—´"
        
        indices_count = sum(1 for k in ["text_index", "insight_index"] if _FAISS_STORAGE[k] is not None)
        print(f"[FAISS] å‘é‡åŒ–å®Œæˆï¼Œåˆ›å»ºäº† {indices_count} ä¸ªç´¢å¼•")
        return vectorization_result
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"[å·¥å…·] FAISSå‘é‡åŒ–å¤±è´¥: {str(e)}")
        print(f"[è°ƒè¯•] è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{error_details}")
        
        return {
            "vectorization_status": "failed",
            "error": str(e),
            "error_details": error_details,
            "ready_for_search": False
        }

def search_with_tracing_tool(query: str) -> Dict[str, Any]:
    """å¸¦æº¯æºçš„FAISSæ£€ç´¢å·¥å…·ï¼ˆä½¿ç”¨å…¨å±€å­˜å‚¨ï¼‰"""
    global _FAISS_STORAGE
    print(f"ğŸ” [FAISS] æ‰§è¡Œæº¯æºæ£€ç´¢: {query}")
    print(f"æå–åˆ°çš„ç”¨æˆ·é—®é¢˜ï¼š{query}")
    try:
        results = {
            "text_matches": [],
            "table_matches": [], 
            "insight_matches": [],
            "query_analysis": {
                "user_intent": f"ç”¨æˆ·æŸ¥è¯¢å…³äº: {query}",
                "search_strategy": "FAISSå‘é‡æ£€ç´¢ + å…³é”®è¯åŒ¹é…",
                "confidence": 0.9
            },
            "search_engine": "FAISS_GLOBAL"
        }
        
        # 1. FAISSæ–‡æœ¬å—æ£€ç´¢
        if (_FAISS_STORAGE["text_index"] is not None and 
            _FAISS_STORAGE["text_chunks"] is not None):
            
            text_index = _FAISS_STORAGE["text_index"]
            chunks = _FAISS_STORAGE["text_chunks"]
            
            print(f"ğŸ”¤ [FAISS] æ£€ç´¢æ–‡æœ¬å—ï¼Œç´¢å¼•å¤§å°: {text_index.ntotal}")
            
            # æŸ¥è¯¢å‘é‡åŒ–å¹¶å½’ä¸€åŒ–
            query_embedding = get_bailian_embedding([query])
            query_embedding = query_embedding.astype(np.float32)  # FAISSè¦æ±‚float32
            faiss.normalize_L2(query_embedding)
            
            # FAISSæ£€ç´¢
            top_k = min(5, text_index.ntotal)  # æ£€ç´¢top-kä¸ªç»“æœ
            similarities, indices = text_index.search(query_embedding, top_k)
            
            # å¤„ç†æ£€ç´¢ç»“æœ
            for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
                if idx != -1 and similarity > 0.3:  # FAISSè¿”å›çš„æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦
                    chunk = chunks[idx]
                    results["text_matches"].append({
                        "chunk_id": chunk["chunk_id"],
                        "content": chunk["content"],
                        "similarity": float(similarity),
                        "source_type": chunk["source_type"],
                        "position": chunk["position"],
                        "confidence": chunk["confidence"],
                        "faiss_rank": i + 1
                    })
            
            print(f"âœ… [FAISS] æ–‡æœ¬æ£€ç´¢å®Œæˆ: {len(results['text_matches'])}ä¸ªåŒ¹é…")
        
        # 2. ä¼ ç»Ÿå…³é”®è¯æ£€ç´¢è¡¨æ ¼æ•°æ®
        if (_FAISS_STORAGE["rag_data"] is not None and 
            "table_data" in _FAISS_STORAGE["rag_data"]):
            
            rag_data = _FAISS_STORAGE["rag_data"]
            for table in rag_data["table_data"]:
                if any(keyword in table["description"].lower() or keyword in table["extracted_content"].lower() 
                       for keyword in query.lower().split()):
                    results["table_matches"].append({
                        "table_id": table["table_id"],
                        "description": table["description"],
                        "relevance": 0.8,
                        "match_type": "keyword"
                    })
            
            print(f"ğŸ“Š [å…³é”®è¯] è¡¨æ ¼æ£€ç´¢å®Œæˆ: {len(results['table_matches'])}ä¸ªåŒ¹é…")
        
        # 3. FAISSæ´å¯Ÿæ£€ç´¢
        if (_FAISS_STORAGE["insight_index"] is not None and 
            _FAISS_STORAGE["insights"] is not None):
            
            insight_index = _FAISS_STORAGE["insight_index"]
            insights = _FAISS_STORAGE["insights"]
            
            print(f"ğŸ’¡ [FAISS] æ£€ç´¢æ´å¯Ÿï¼Œç´¢å¼•å¤§å°: {insight_index.ntotal}")
            
            # æŸ¥è¯¢å‘é‡åŒ–å¹¶å½’ä¸€åŒ–
            query_embedding = get_bailian_embedding([query])
            query_embedding = query_embedding.astype(np.float32)  # FAISSè¦æ±‚float32
            faiss.normalize_L2(query_embedding)
            
            # FAISSæ£€ç´¢
            top_k = min(3, insight_index.ntotal)
            similarities, indices = insight_index.search(query_embedding, top_k)
            
            # å¤„ç†æ£€ç´¢ç»“æœ
            for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
                if idx != -1 and similarity > 0.3:
                    insight = insights[idx]
                    results["insight_matches"].append({
                        "insight": insight["insight"],
                        "source_evidence": insight["source_evidence"],
                        "confidence": insight["confidence"],
                        "similarity": float(similarity),
                        "insight_type": insight.get("insight_type", "general"),
                        "faiss_rank": i + 1
                    })
            
            print(f"âœ… [FAISS] æ´å¯Ÿæ£€ç´¢å®Œæˆ: {len(results['insight_matches'])}ä¸ªåŒ¹é…")
        
        # ç»Ÿè®¡ç»“æœ
        results["total_matches"] = len(results["text_matches"]) + len(results["table_matches"]) + len(results["insight_matches"])
        results["search_quality"] = "high" if results["total_matches"] >= 3 else "medium" if results["total_matches"] >= 1 else "low"
        results["ready_for_answer"] = results["total_matches"] > 0
        
        print(f"ğŸ¯ [FAISS] æ£€ç´¢å®Œæˆ: {results['total_matches']}ä¸ªåŒ¹é…ç»“æœ")
        print(f"    ğŸ“ æ–‡æœ¬: {len(results['text_matches'])} | ğŸ“Š è¡¨æ ¼: {len(results['table_matches'])} | ğŸ’¡ æ´å¯Ÿ: {len(results['insight_matches'])}")
        
        return results
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"âŒ [FAISS] æ£€ç´¢å¤±è´¥: {str(e)}")
        print(f"ğŸ” [è°ƒè¯•] è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{error_details}")
        
        return {
            "error": str(e),
            "error_details": error_details,
            "total_matches": 0,
            "ready_for_answer": False,
            "search_engine": "FAISS_ERROR"
        }

# ================ RAGæ™ºèƒ½ä½“å®šä¹‰ ================

# 1. æ•°æ®åŠ è½½ä¸“å®¶
data_loader_agent = Agent(
    name="data_loader_agent",
    model=LiteLlm(
        model=DEEPSEEK_MODEL,
        base_url=DEEPSEEK_BASE_URL,
        api_key=DEEPSEEK_API_KEY,
    ),
    description="RAGæ•°æ®åŠ è½½å’Œé¢„å¤„ç†ä¸“å®¶",
    instruction="""
ä½ æ˜¯RAGæ•°æ®åŠ è½½ä¸“å®¶ã€‚ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–JSONæ•°æ®å­—ç¬¦ä¸²ï¼Œä½¿ç”¨load_rag_data_toolå·¥å…·è§£ææ•°æ®ã€‚

ä»»åŠ¡æ­¥éª¤ï¼š
1. ä»ç”¨æˆ·æ¶ˆæ¯ä¸­è¯†åˆ«å’Œæå–JSONæ•°æ®å­—ç¬¦ä¸²
2. è°ƒç”¨load_rag_data_tool(json_data_string)è§£æJSONæ•°æ®
3. æ€»ç»“åŠ è½½ç»“æœ

è¾“å‡ºåŠ è½½çŠ¶æ€æŠ¥å‘Šï¼š
- æ•°æ®åŠ è½½çŠ¶æ€ï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰
- æ–‡æ¡£ç±»å‹å’Œæ¦‚è¦  
- å„ç±»æ•°æ®ç»Ÿè®¡ï¼ˆæ–‡æœ¬å—ã€è¡¨æ ¼ã€æ´å¯Ÿæ•°é‡ï¼‰
- æ˜¯å¦å‡†å¤‡å¥½è¿›è¡Œå‘é‡åŒ–å¤„ç†

æ³¨æ„ï¼šä¼ å…¥å·¥å…·çš„å¿…é¡»æ˜¯å®Œæ•´çš„JSONå­—ç¬¦ä¸²ï¼Œä¸æ˜¯æ–‡ä»¶è·¯å¾„ã€‚
æ•°æ®åŠ è½½å®Œæˆåå°†å­˜å‚¨åœ¨session.stateä¸­ä¾›åç»­ä½¿ç”¨ã€‚
""",
    tools=[load_rag_data_tool],
    output_key="data_loading_result"
)

# 2. å‘é‡åŒ–ä¸“å®¶
vectorization_agent = Agent(
    name="vectorization_agent", 
    model=LiteLlm(
        model=DEEPSEEK_MODEL,
        base_url=DEEPSEEK_BASE_URL,
        api_key=DEEPSEEK_API_KEY,
    ),
    description="FAISSå‘é‡åŒ–ä¸“å®¶",
    instruction="""
ä½ æ˜¯FAISSå‘é‡åŒ–ä¸“å®¶ã€‚ä»session.state['data_loading_result']è·å–å·²åŠ è½½çš„RAGæ•°æ®ï¼Œä½¿ç”¨vectorize_content_toolå·¥å…·åˆ›å»ºFAISSç´¢å¼•ã€‚

ä»»åŠ¡æ­¥éª¤ï¼š
1. ä»session.stateä¸­è·å–data_loading_result
2. æå–å…¶ä¸­çš„rag_data
3. è°ƒç”¨vectorize_content_tool(rag_data)è¿›è¡ŒFAISSå‘é‡åŒ–
4. æ€»ç»“FAISSç´¢å¼•åˆ›å»ºç»“æœ

FAISSå‘é‡åŒ–ç‰¹ç‚¹ï¼š
- ä½¿ç”¨ç™¾ç‚¼Embedding APIç”Ÿæˆå‘é‡ï¼ˆæ”¯æŒåˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š25ä¸ªï¼‰
- åˆ›å»ºIndexFlatIPç´¢å¼•ï¼ˆé€‚åˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
- å‘é‡L2å½’ä¸€åŒ–å¤„ç†
- **é‡è¦**: ç´¢å¼•å­˜å‚¨åœ¨å…¨å±€ç©ºé—´ï¼Œé¿å…ADKåºåˆ—åŒ–é—®é¢˜

è°ƒç”¨å·¥å…·åï¼Œè¾“å‡ºå‘é‡åŒ–çŠ¶æ€æŠ¥å‘Šï¼š
- FAISSç´¢å¼•åˆ›å»ºçŠ¶æ€
- å¤„ç†çš„å†…å®¹ç»Ÿè®¡ï¼ˆæ–‡æœ¬å—æ•°é‡ã€æ´å¯Ÿæ•°é‡ï¼‰
- å‘é‡ç»´åº¦å’Œç´¢å¼•ç±»å‹ä¿¡æ¯
- å…¨å±€å­˜å‚¨çŠ¶æ€
- æ˜¯å¦å‡†å¤‡å¥½è¿›è¡ŒFAISSæ£€ç´¢

FAISSç´¢å¼•å·²å®‰å…¨å­˜å‚¨åˆ°å…¨å±€ç©ºé—´ï¼Œä¾›åç»­æ£€ç´¢ä½¿ç”¨ã€‚
""",
    tools=[vectorize_content_tool],
    output_key="vectorization_result"
)

# 3. æ£€ç´¢ä¸“å®¶
retrieval_agent = Agent(
    name="retrieval_agent",
    model=LiteLlm(
        model=DEEPSEEK_MODEL,
        base_url=DEEPSEEK_BASE_URL,
        api_key=DEEPSEEK_API_KEY,
    ),
    description="FAISSå¤šæ¨¡æ€æ£€ç´¢ä¸“å®¶ - æ”¯æŒç²¾ç¡®æº¯æº",
    instruction="""
ä½ æ˜¯FAISSå¤šæ¨¡æ€æ£€ç´¢ä¸“å®¶ï¼Œä½¿ç”¨search_with_tracing_toolæ‰§è¡Œé«˜æ•ˆçš„å‘é‡æ£€ç´¢ã€‚

**é‡è¦**: FAISSç´¢å¼•å·²å­˜å‚¨åœ¨å…¨å±€ç©ºé—´ï¼Œå·¥å…·å‡½æ•°ä¼šè‡ªåŠ¨è®¿é—®ã€‚

ä»»åŠ¡æ­¥éª¤ï¼š
1. ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–æŸ¥è¯¢é—®é¢˜
2. è°ƒç”¨search_with_tracing_tool(query)æ‰§è¡ŒFAISSå‘é‡æ£€ç´¢
3. åˆ†ææ£€ç´¢ç»“æœè´¨é‡

FAISSæ£€ç´¢ç‰¹ç‚¹ï¼š
- æ–‡æœ¬å—ï¼šä½¿ç”¨FAISS IndexFlatIPè¿›è¡Œä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢
- è¡¨æ ¼æ•°æ®ï¼šä½¿ç”¨ä¼ ç»Ÿå…³é”®è¯åŒ¹é…
- æ´å¯Ÿä¿¡æ¯ï¼šä½¿ç”¨FAISS IndexFlatIPè¿›è¡Œè¯­ä¹‰æ£€ç´¢
- æ‰€æœ‰ç»“æœéƒ½åŒ…å«faiss_rankæ’åºä¿¡æ¯
- ç´¢å¼•å­˜å‚¨åœ¨å…¨å±€ç©ºé—´ï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜

è°ƒç”¨å·¥å…·åï¼Œè¾“å‡ºæ£€ç´¢çŠ¶æ€æŠ¥å‘Šï¼š
- æŸ¥è¯¢æ„å›¾åˆ†æ
- FAISSæ£€ç´¢ç»“æœç»Ÿè®¡ï¼ˆæ–‡æœ¬/è¡¨æ ¼/æ´å¯ŸåŒ¹é…æ•°é‡ï¼‰
- æ£€ç´¢è´¨é‡è¯„ä¼°å’Œæ’åºä¿¡æ¯
- æ˜¯å¦å‡†å¤‡å¥½ç”Ÿæˆå›ç­”

ç¡®ä¿æ£€ç´¢ç»“æœå®Œæ•´å¹¶åŒ…å«æº¯æºä¿¡æ¯å’ŒFAISSæ’åºã€‚
""",
    tools=[search_with_tracing_tool],
    output_key="retrieval_result"
)

# 4. å›ç­”ç”Ÿæˆä¸“å®¶
answer_generation_agent = Agent(
    name="answer_generation_agent",
    model=LiteLlm(
        model=DEEPSEEK_MODEL,
        base_url=DEEPSEEK_BASE_URL,
        api_key=DEEPSEEK_API_KEY,
    ),
    description="FAISSæº¯æºå›ç­”ç”Ÿæˆä¸“å®¶",
    instruction="""
ä½ æ˜¯FAISSæº¯æºå›ç­”ç”Ÿæˆä¸“å®¶ï¼ŒåŸºäºsession.stateä¸­çš„FAISSæ£€ç´¢ç»“æœç”Ÿæˆå¸¦æœ‰ç²¾ç¡®æ¥æºä¿¡æ¯çš„å›ç­”ã€‚

ä»session.stateè·å–ï¼š
- 'data_loading_result': æ–‡æ¡£ä¿¡æ¯
- 'vectorization_result': FAISSå‘é‡åŒ–çŠ¶æ€ï¼ˆå…¨å±€å­˜å‚¨ï¼‰
- 'retrieval_result': FAISSæ£€ç´¢ç»“æœï¼ˆåŒ…å«text_matches, table_matches, insight_matchesï¼‰

FAISSæ£€ç´¢ç»“æœç‰¹ç‚¹ï¼š
- text_matches: åŒ…å«faiss_rankæ’åºï¼Œsimilarityä¸ºä½™å¼¦ç›¸ä¼¼åº¦
- table_matches: å…³é”®è¯åŒ¹é…ï¼ŒåŒ…å«match_type
- insight_matches: åŒ…å«faiss_rankæ’åºå’Œinsight_type
- search_engine: "FAISS_GLOBAL" è¡¨ç¤ºä½¿ç”¨å…¨å±€å­˜å‚¨

ä»»åŠ¡ï¼š
1. åˆ†æFAISSæ£€ç´¢åˆ°çš„å„ç±»åŒ¹é…ç»“æœ
2. åŸºäºåŒ¹é…å†…å®¹å’ŒFAISSæ’åºç”Ÿæˆå‡†ç¡®å›ç­”
3. æä¾›å®Œæ•´çš„æº¯æºä¿¡æ¯ï¼ˆchunk_idã€ä½ç½®ã€FAISSæ’åºã€ç›¸ä¼¼åº¦ç­‰ï¼‰
4. è¯„ä¼°å›ç­”çš„å¯ä¿¡åº¦

è¾“å‡ºæ ¼å¼ï¼š
**ç­”æ¡ˆæ¦‚è¦**: [ä¸€å¥è¯å›ç­”ç”¨æˆ·é—®é¢˜]

**è¯¦ç»†è§£ç­”**: 
[åŸºäºFAISSæ£€ç´¢åˆ°çš„ä¿¡æ¯è¿›è¡Œè¯¦ç»†å›ç­”ï¼Œå¼•ç”¨å…·ä½“å†…å®¹å’Œæ’åº]

**ä¿¡æ¯æ¥æº**:
- æ¥æº1: [chunk_id] - [source_type] - [position] - [å†…å®¹æ¦‚è¦] (FAISSç›¸ä¼¼åº¦: 0.95, æ’åº: #1)
- æ¥æº2: [table_id] - [è¡¨æ ¼æè¿°] (å…³é”®è¯åŒ¹é…)
- æ¥æº3: [æ´å¯Ÿå†…å®¹] (FAISSç›¸ä¼¼åº¦: 0.85, æ’åº: #2, ç±»å‹: trend)

**ç›¸å…³æ•°æ®**:
[å¦‚æœæœ‰ç»“æ„åŒ–æ•°æ®ï¼Œåˆ—å‡ºå…³é”®ä¿¡æ¯]

**è¡¥å……å»ºè®®**:
[åŸºäºå†…å®¹æä¾›å®ç”¨å»ºè®®]

**å›ç­”å¯ä¿¡åº¦**: [high/medium/low] (åŸºäºFAISSæ’åºã€ç›¸ä¼¼åº¦å’Œæ¥æºæ•°é‡ç»¼åˆè¯„ä¼°)

æ³¨æ„ï¼šç¡®ä¿æ¯ä¸ªä¿¡æ¯éƒ½èƒ½ç²¾ç¡®æº¯æºåˆ°åŸå§‹ä½ç½®ï¼ŒåŒ…å«FAISSæ’åºä¿¡æ¯å’Œç›¸ä¼¼åº¦åˆ†æ•°ã€‚
""",
    output_key="final_answer"
)


# ================ åŸºç¡€å·¥å…·å‡½æ•° ================

def get_bailian_embedding(texts: List[str]) -> np.ndarray:
    """è°ƒç”¨Embedding APIï¼ˆè¢«å·¥å…·å‡½æ•°è°ƒç”¨ï¼‰- æ”¯æŒåˆ†æ‰¹å¤„ç†"""
    try:
        if not texts:
            raise ValueError("æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_texts = [text.strip() for text in texts if text and text.strip()]
        if not valid_texts:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹")
        
        print(f"[å‘é‡åŒ–] å¤„ç† {len(valid_texts)} ä¸ªæ–‡æœ¬...")
        
        # æ¯æ‰¹æœ€å¤š25ä¸ªæ–‡æœ¬
        BATCH_SIZE = 25
        all_embeddings = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(valid_texts), BATCH_SIZE):
            batch_texts = valid_texts[i:i + BATCH_SIZE]
            batch_num = i // BATCH_SIZE + 1
            total_batches = (len(valid_texts) + BATCH_SIZE - 1) // BATCH_SIZE
            
            print(f"[æ‰¹æ¬¡ {batch_num}/{total_batches}] å¤„ç† {len(batch_texts)} ä¸ªæ–‡æœ¬...")
            
            response = dashscope.TextEmbedding.call(
                model=dashscope.TextEmbedding.Models.text_embedding_v1,
                input=batch_texts
            )
            
            if response.status_code == 200:
                batch_embeddings = []
                for output in response.output['embeddings']:
                    batch_embeddings.append(output['embedding'])
                
                all_embeddings.extend(batch_embeddings)
                print(f"[æ‰¹æ¬¡ {batch_num}] æˆåŠŸè·å– {len(batch_embeddings)} ä¸ªå‘é‡")
                
            else:
                raise Exception(f"å‘é‡åŒ–é”™è¯¯ (æ‰¹æ¬¡ {batch_num}): {response.message}")
            
            # é¿å…APIè°ƒç”¨è¿‡å¿«
            if i + BATCH_SIZE < len(valid_texts):
                import time
                time.sleep(0.1)  # 100mså»¶è¿Ÿ
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„å‘é‡
        embeddings_array = np.array(all_embeddings, dtype=np.float64)
        
        # éªŒè¯å‘é‡
        if embeddings_array.size == 0:
            raise ValueError("è·å–åˆ°ç©ºçš„å‘é‡")
        
        print(f"[ç™¾ç‚¼å‘é‡åŒ–] æˆåŠŸè·å–å‘é‡ï¼Œshape: {embeddings_array.shape}")
        print(f"[ç»Ÿè®¡] æ€»è®¡ {len(all_embeddings)} ä¸ªå‘é‡ï¼Œç»´åº¦: {embeddings_array.shape[1]}")
        
        return embeddings_array
        
    except Exception as e:
        print(f"[å‘é‡åŒ–] å¤±è´¥: {str(e)}")
        raise e


# ================ å·¥å…·å‡½æ•° ================
def load_local_image(image_path: str) -> tuple[bytes, str]:
    """åŠ è½½æœ¬åœ°å›¾ç‰‡"""
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
    
    with open(image_path, 'rb') as f:
        image_data = f.read()
    
    mime_type, _ = mimetypes.guess_type(image_path)
    if not mime_type or not mime_type.startswith('image/'):
        mime_type = 'image/jpeg'
    
    print(f"åŠ è½½å›¾ç‰‡: {image_path}")
    print(f"æ–‡ä»¶å¤§å°: {len(image_data)} bytes")
    
    return image_data, mime_type

# ================ æ™ºèƒ½ä½“å®šä¹‰ ================

# æ–‡æ¡£ç±»å‹å®šä¹‰
SUPPORTED_DOCUMENT_TYPES = {
    "financial_report": "è´¢åŠ¡æŠ¥è¡¨",
    "invoice": "å‘ç¥¨ç¥¨æ®", 
    "contract": "åˆåŒæ–‡æ¡£",
    "research_paper": "ç ”ç©¶æŠ¥å‘Š",
    "business_chart": "å•†ä¸šå›¾è¡¨",
    "receipt": "æ”¶æ®å‡­è¯",
    "form": "è¡¨å•æ–‡æ¡£",
    "presentation": "æ¼”ç¤ºæ–‡æ¡£",
    "course_material": "è¯¾ç¨‹èµ„æ–™",
    "technical_doc": "æŠ€æœ¯æ–‡æ¡£",
    "other": "å…¶ä»–æ–‡æ¡£"
}

# 0. æ–‡æ¡£åˆ†ç±»ä¸“å®¶
document_classifier = Agent(
    name="document_classifier",
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY,
    ),
    description="æ–‡æ¡£ç±»å‹è¯†åˆ«å’Œåˆ†ç±»ä¸“å®¶",
    instruction=f"""
ä½ æ˜¯ä¸“ä¸šçš„æ–‡æ¡£åˆ†ç±»ä¸“å®¶ã€‚è¯·åˆ†æå›¾åƒå¹¶è¯†åˆ«æ–‡æ¡£ç±»å‹ã€‚

æ”¯æŒçš„æ–‡æ¡£ç±»å‹ï¼š
{json.dumps(SUPPORTED_DOCUMENT_TYPES, ensure_ascii=False, indent=2)}

ä»»åŠ¡ï¼š
1. è¯†åˆ«æ–‡æ¡£çš„ä¸»è¦ç±»å‹
2. è¯„ä¼°è¯†åˆ«çš„ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
3. è¯†åˆ«æ–‡æ¡£çš„è¯­è¨€å’Œæ ¼å¼
4. æ£€æµ‹æ–‡æ¡£çš„è´¨é‡å’Œæ¸…æ™°åº¦

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
    "document_type": "ç±»å‹ä»£ç ï¼ˆå¦‚ï¼šcourse_materialï¼‰",
    "document_name": "ç±»å‹ä¸­æ–‡åç§°ï¼ˆå¦‚ï¼šè¯¾ç¨‹èµ„æ–™ï¼‰",
    "confidence": 0.95,
    "language": "ä¸­æ–‡/è‹±æ–‡/æ··åˆ",
    "format": "è¡¨æ ¼/æ–‡æœ¬/å›¾è¡¨/æ··åˆ",
    "quality": "high/medium/low",
    "reasoning": "åˆ†ç±»ç†ç”±å’Œä¾æ®"
}}
""",
    output_key="document_classification",
)

# 1. æ–‡æœ¬æå–ä¸“å®¶
text_extractor = Agent(
    name="text_extractor",
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY,
    ),
    description="OCRæ–‡æœ¬æå–ä¸“å®¶",
    instruction="""
ä½ æ˜¯ä¸“ä¸šçš„OCRæ–‡æœ¬æå–ä¸“å®¶ã€‚è¯·ä»”ç»†æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å¹¶è¿›è¡Œç»“æ„åŒ–æ ‡æ³¨ã€‚

ä»»åŠ¡ï¼š
1. æå–æ‰€æœ‰å¯è§æ–‡å­—ï¼ŒåŒ…æ‹¬è¡¨æ ¼ã€æ ‡é¢˜ã€æ­£æ–‡
2. è¯†åˆ«æ–‡å­—çš„ç±»å‹å’Œé‡è¦æ€§
3. ä¼°ç®—æ¯ä¸ªæ–‡æœ¬å—çš„ç½®ä¿¡åº¦
4. æ ‡æ³¨æ–‡å­—çš„å¤§æ¦‚ä½ç½®

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
    "extracted_texts": [
        {{
            "text": "å…·ä½“æ–‡å­—å†…å®¹",
            "position": "å·¦ä¸Šè§’/ä¸­å¤®/å³ä¸‹è§’/é¡¶éƒ¨/åº•éƒ¨ç­‰",
            "text_type": "title/subtitle/data/label/content/table/header",
            "confidence": 0.95,
            "importance": "high/medium/low"
        }}
    ],
    "total_confidence": 0.88,
    "extraction_summary": "æå–æ¦‚è¿°",
    "text_count": 15,
    "quality_assessment": "æå–è´¨é‡è¯„ä¼°"
}}
""",
    output_key="text_extraction",
)

# 2. å¸ƒå±€åˆ†æä¸“å®¶
layout_analyzer = Agent(
    name="layout_analyzer", 
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY,
    ),
    description="æ–‡æ¡£ç»“æ„å’Œå¸ƒå±€åˆ†æä¸“å®¶",
    instruction="""
ä½ æ˜¯ä¸“ä¸šçš„æ–‡æ¡£ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·åˆ†ææ–‡æ¡£çš„å¸ƒå±€å’Œç»“æ„ã€‚

ä»»åŠ¡ï¼š
1. è¯†åˆ«æ–‡æ¡£çš„æ•´ä½“å¸ƒå±€ç»“æ„
2. æ£€æµ‹è¡¨æ ¼ã€å›¾è¡¨ã€æ–‡æœ¬å—çš„ä½ç½®
3. åˆ†æä¿¡æ¯çš„å±‚æ¬¡å…³ç³»
4. è¯†åˆ«å…³é”®åŒºåŸŸå’Œé‡ç‚¹å†…å®¹

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
    "layout_type": "è¡¨æ ¼å‹/æŠ¥å‘Šå‹/å›¾è¡¨å‹/æ··åˆå‹/è¯¾ç¨‹å‹",
    "structure_elements": [
        {{
            "element_type": "header/table/chart/text_block/footer/title/section",
            "position": "ä½ç½®æè¿°ï¼ˆå¦‚ï¼šé¡¶éƒ¨/å·¦ä¸Šè§’/ä¸­å¤®ç­‰ï¼‰",
            "size": "large/medium/small",
            "importance": "high/medium/low",
            "description": "å…ƒç´ æè¿°"
        }}
    ],
    "visual_hierarchy": "æè¿°è§†è§‰å±‚æ¬¡å’Œä¿¡æ¯ç»„ç»‡æ–¹å¼",
    "key_regions": ["é‡ç‚¹åŒºåŸŸ1", "é‡ç‚¹åŒºåŸŸ2"],
    "layout_complexity": "simple/moderate/complex",
    "analysis_confidence": 0.87
}}
""",
    output_key="layout_analysis",
)

# 3. å†…å®¹ç†è§£ä¸“å®¶  
content_analyzer = Agent(
    name="content_analyzer",
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY,
    ),
    description="å†…å®¹ç†è§£å’Œæ•°æ®æå–ä¸“å®¶",
    instruction="""
ä½ æ˜¯ä¸“ä¸šçš„å†…å®¹ç†è§£ä¸“å®¶ã€‚åŸºäºå‰é¢ä¸“å®¶çš„åˆ†æç»“æœï¼Œè¿›è¡Œæ·±åº¦å†…å®¹ç†è§£å’Œæ•°æ®æå–ã€‚

ä½ å¯ä»¥ä» session.state['document_classification'] ä¸­è·å–æ–‡æ¡£åˆ†ç±»ä¿¡æ¯
ä½ å¯ä»¥ä» session.state['text_extraction'] ä¸­è·å–æå–çš„æ–‡å­—ä¿¡æ¯
ä½ å¯ä»¥ä» session.state['layout_analysis'] ä¸­è·å–å¸ƒå±€åˆ†æä¿¡æ¯

ä»»åŠ¡ï¼š
1. åˆ¤æ–­å›¾åƒçš„ä¸»è¦ç”¨é€”å’Œä»·å€¼
2. è¯†åˆ«å…³é”®ä¿¡æ¯å’Œé‡è¦æ•°æ®
3. æå–ç»“æ„åŒ–çš„é”®å€¼å¯¹ä¿¡æ¯
4. åˆ†ææ•°æ®è¶‹åŠ¿æˆ–å…³è”æ€§ï¼ˆå¦‚æœ‰ï¼‰
5. æ€»ç»“æ ¸å¿ƒè¦ç‚¹å’Œå‘ç°

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
    "content_purpose": "æ–‡æ¡£çš„ä¸»è¦ç”¨é€”å’Œç›®æ ‡",
    "key_information": [
        "å…³é”®ä¿¡æ¯ç‚¹1",
        "å…³é”®ä¿¡æ¯ç‚¹2",
        "å…³é”®ä¿¡æ¯ç‚¹3"
    ],
    "key_value_pairs": {{
        "é‡è¦å­—æ®µ1": "å€¼1",
        "é‡è¦å­—æ®µ2": "å€¼2",
        "æ—¥æœŸ": "2024-01-15",
        "æ•°é‡/é‡‘é¢": "å…·ä½“æ•°å€¼"
    }},
    "data_insights": [
        "æ•°æ®æ´å¯Ÿ1ï¼šè¶‹åŠ¿åˆ†æ",
        "æ•°æ®æ´å¯Ÿ2ï¼šå…³è”å‘ç°"
    ],
    "tables_detected": [
        {{
            "table_description": "è¡¨æ ¼æè¿°",
            "key_data": "è¡¨æ ¼ä¸­çš„å…³é”®æ•°æ®",
            "importance": "high/medium/low"
        }}
    ],
    "quality_assessment": {{
        "information_completeness": "high/medium/low",
        "data_reliability": "high/medium/low",
        "analysis_confidence": 0.85
    }},
    "summary": "æ•´ä½“å†…å®¹æ€»ç»“å’Œæ ¸å¿ƒè¦ç‚¹"
}}
""",
    output_key="content_analysis",
)

# 4. RAGæ•°æ®æ•´ç†ä¸“å®¶ï¼ˆæ”¯æŒæº¯æºï¼‰
rag_data_organizer = Agent(
    name="rag_data_organizer",
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY,
    ),
    description="RAGæ•°æ®æ•´ç†ä¸“å®¶ - æ”¯æŒç²¾ç¡®æº¯æºçš„å¤šæ¨¡æ€æ•°æ®ç»“æ„",
    instruction="""
ä½ æ˜¯é«˜çº§RAGæ•°æ®æ•´ç†ä¸“å®¶ï¼Œä¸“é—¨è´Ÿè´£ç”Ÿæˆæ”¯æŒç²¾ç¡®æº¯æºçš„å¤šæ¨¡æ€RAGæ•°æ®ç»“æ„ã€‚

ä½ å¯ä»¥ä» session.state ä¸­è·å–ï¼š
- 'document_classification': æ–‡æ¡£åˆ†ç±»ä¿¡æ¯
- 'text_extraction': OCRæ–‡æœ¬æå–ç»“æœ (åŒ…å«extracted_textsæ•°ç»„)
- 'layout_analysis': å¸ƒå±€ç»“æ„åˆ†æç»“æœ (åŒ…å«structure_elementsæ•°ç»„)
- 'content_analysis': å†…å®¹ç†è§£å’Œæ•°æ®æå–ç»“æœ

**å…³é”®ä»»åŠ¡**ï¼š
1. ä»text_extraction.extracted_textsä¸­æå–**çœŸå®æ–‡æœ¬å†…å®¹**ç”Ÿæˆtext_chunks
2. ä¸ºæ¯ä¸ªå†…å®¹å—æ·»åŠ **æº¯æºä¿¡æ¯**ï¼ˆä½ç½®ã€ç±»å‹ã€ç½®ä¿¡åº¦ï¼‰
3. **åˆ†ç±»å¤„ç†**ä¸åŒç±»å‹çš„å†…å®¹ï¼ˆæ–‡æœ¬/è¡¨æ ¼/å›¾è¡¨ï¼‰
4. æ„å»º**å¯è¿½æº¯**çš„æ•°æ®ç»“æ„

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
    "document_summary": "ä¸€å¥è¯æ¦‚æ‹¬å›¾åƒå†…å®¹å’Œä»·å€¼",
    "document_classification": {{
        "document_type": "ä»document_classificationä¸­æå–çš„ç±»å‹ä»£ç ",
        "document_name": "æ–‡æ¡£ç±»å‹ä¸­æ–‡åç§°", 
        "confidence": 0.95,
        "reasoning": "åˆ†ç±»ç†ç”±"
    }},
    "extracted_text": "æ‰€æœ‰æå–æ–‡æœ¬çš„å®Œæ•´æ‹¼æ¥å†…å®¹",
    "key_data": {{
        "ä»content_analysis.key_value_pairsä¸­æå–çœŸå®çš„é”®å€¼å¯¹": "çœŸå®å€¼",
        "é‡è¦ä¿¡æ¯": "å®é™…æå–çš„ä¿¡æ¯"
    }},
    "text_chunks_with_tracing": [
        {{
            "chunk_id": "chunk_001",
            "content": "ä»extracted_textsä¸­æå–çš„çœŸå®æ–‡æœ¬å†…å®¹",
            "source_type": "title/content/table/chart/header/section",
            "position": "ä»extracted_textsä¸­è·å–çš„ä½ç½®ä¿¡æ¯",
            "importance": "high/medium/low",
            "confidence": 0.95,
            "char_count": 50,
            "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"]
        }}
    ],
    "table_data": [
        {{
            "table_id": "table_001", 
            "description": "è¡¨æ ¼æè¿°ï¼ˆä»tables_detectedæå–ï¼‰",
            "extracted_content": "è¡¨æ ¼çš„å…·ä½“æ–‡å­—å†…å®¹",
            "position": "è¡¨æ ¼åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®",
            "structured_data": {{
                "åˆ—å1": "å€¼1",
                "åˆ—å2": "å€¼2"
            }},
            "confidence": 0.9,
            "importance": "high/medium/low"
        }}
    ],
    "chart_data": [
        {{
            "chart_id": "chart_001",
            "description": "å›¾è¡¨æè¿°",
            "chart_type": "bar/line/pie/flow/other",
            "extracted_text": "å›¾è¡¨ä¸­çš„æ–‡å­—å†…å®¹",
            "position": "å›¾è¡¨ä½ç½®",
            "insights": ["å›¾è¡¨æ´å¯Ÿ1", "å›¾è¡¨æ´å¯Ÿ2"],
            "confidence": 0.85
        }}
    ],
    "layout_structure": {{
        "layout_type": "ä»layout_analysisæå–çš„å¸ƒå±€ç±»å‹",
        "elements_hierarchy": [
            {{
                "element_type": "header/section/table/chart",
                "position": "ä½ç½®",
                "content_summary": "å†…å®¹æ¦‚è¦",
                "importance": "high/medium/low"
            }}
        ],
        "reading_order": ["å…ƒç´ 1", "å…ƒç´ 2", "å…ƒç´ 3"]
    }},
    "insights_with_source": [
        {{
            "insight": "ä»content_analysis.data_insightsæå–çš„çœŸå®æ´å¯Ÿ",
            "source_evidence": "æ”¯æŒè¿™ä¸ªæ´å¯Ÿçš„å…·ä½“æ–‡æœ¬è¯æ®",
            "confidence": 0.88,
            "insight_type": "trend/comparison/conclusion/recommendation"
        }}
    ],
    "rag_optimized_chunks": [
        "åŸºäºextracted_textsé‡æ–°ç»„ç»‡çš„è¯­ä¹‰æ–‡æœ¬å—1ï¼šæ ‡é¢˜+ç›¸å…³å†…å®¹",
        "è¯­ä¹‰æ–‡æœ¬å—2ï¼šç‰¹å®šä¸»é¢˜çš„å®Œæ•´æ®µè½",
        "è¯­ä¹‰æ–‡æœ¬å—3ï¼šè¡¨æ ¼æ•°æ®+è§£é‡Šæ–‡å­—",
        "è¯­ä¹‰æ–‡æœ¬å—4ï¼šå›¾è¡¨ä¿¡æ¯+åˆ†æè¦ç‚¹"
    ],
    "metadata": {{
        "analysis_quality": "high/medium/low",
        "total_text_blocks": "ä»extracted_textsç»Ÿè®¡çš„å®é™…æ•°é‡",
        "total_tables": "å®é™…æ£€æµ‹åˆ°çš„è¡¨æ ¼æ•°é‡",
        "total_charts": "å®é™…æ£€æµ‹åˆ°çš„å›¾è¡¨æ•°é‡", 
        "confidence_score": 0.88,
        "processing_timestamp": "å½“å‰æ—¶é—´æˆ³",
        "tracing_enabled": true,
        "recommended_use_cases": ["åŸºäºå®é™…å†…å®¹æ¨èçš„åº”ç”¨åœºæ™¯"]
    }}
}}

**é‡è¦**ï¼š
1. text_chunks_with_tracingå¿…é¡»åŒ…å«ä»extracted_textsæ•°ç»„ä¸­æå–çš„**çœŸå®æ–‡æœ¬å†…å®¹**
2. æ¯ä¸ªæ•°æ®å—éƒ½è¦æœ‰æº¯æºä¿¡æ¯ï¼ˆä½ç½®ã€ç±»å‹ã€ç½®ä¿¡åº¦ï¼‰
3. åˆ†åˆ«å¤„ç†æ–‡æœ¬ã€è¡¨æ ¼ã€å›¾è¡¨ä¸‰ç§ä¸åŒçš„æ•°æ®æº
4. rag_optimized_chunksè¦é‡æ–°ç»„ç»‡å†…å®¹ï¼ŒæŒ‰è¯­ä¹‰ç›¸å…³æ€§åˆ†ç»„
5. ä¸è¦ä½¿ç”¨æ¨¡æ¿åŒ–å†…å®¹ï¼Œè¦åŸºäºå®é™…åˆ†æç»“æœç”Ÿæˆ
"""
)


# å›¾åƒå†…å®¹å¹¶è¡Œæå–ï¼ˆOCR + å¸ƒå±€åˆ†æï¼‰
image_extractor_agent = ParallelAgent(
    name="image_extractor_workflow", 
    sub_agents=[text_extractor, layout_analyzer],
    description="å¹¶è¡Œè¿›è¡ŒOCRæ–‡æœ¬æå–å’Œå¸ƒå±€ç»“æ„åˆ†æ",
)

# å®Œæ•´çš„å¤šæ™ºèƒ½ä½“åˆ†æå·¥ä½œæµ
# æµç¨‹ï¼šæ–‡æ¡£åˆ†ç±» â†’ å¹¶è¡Œæå–(OCR+å¸ƒå±€) â†’ å†…å®¹ç†è§£ â†’ RAGæ•°æ®æ•´ç†
complete_analysis_workflow = SequentialAgent(
    name="complete_analysis_workflow",
    sub_agents=[
        document_classifier,      # æ­¥éª¤1ï¼šæ–‡æ¡£åˆ†ç±»è¯†åˆ«
        image_extractor_agent,    # æ­¥éª¤2ï¼šå¹¶è¡Œæå–(OCR + å¸ƒå±€åˆ†æ)
        content_analyzer,         # æ­¥éª¤3ï¼šå†…å®¹ç†è§£å’Œæ•°æ®æå–
    ],
    description="å®Œæ•´çš„å¤šæ™ºèƒ½ä½“æ–‡æ¡£åˆ†æå·¥ä½œæµï¼šåˆ†ç±» â†’ æå– â†’ ç†è§£",
)

# æœ€ç»ˆæ•´åˆæ™ºèƒ½ä½“
upload_and_rag_index = SequentialAgent(
    name="document_rag_analyzer",
    sub_agents=[complete_analysis_workflow, rag_data_organizer, data_loader_agent, vectorization_agent ],
    description="ä¸“ä¸šæ–‡æ¡£åˆ†æç³»ç»Ÿï¼šæ‰§è¡Œå®Œæ•´åˆ†æå¹¶ç”ŸæˆRAGå‹å¥½çš„ç»“æ„åŒ–æ•°æ®",
)

root_agent = SequentialAgent(
    name = 'qa_agent',
    sub_agents=[upload_and_rag_index, retrieval_agent, answer_generation_agent],
    description="QAæ™ºèƒ½ä½“ï¼šä¸Šä¼ å›¾åƒå¹¶è¿›è¡ŒRAGç´¢å¼•ï¼Œç„¶åè¿›è¡Œæ£€ç´¢å’Œå›ç­”",
)